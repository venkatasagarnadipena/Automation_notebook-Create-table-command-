{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "258700dd-d40a-4737-be78-1f695b643710",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Script for Automating Delta Table Creation\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "1. **Column Names:** A list of column names.\n",
    "2. **Schema List:** The schema list of the columns provided in the column list.\n",
    "3. **Nullable List:** Specifies constraints for any column. If not applicable, provide an empty list.\n",
    "4. **Table Name:** The name of the table where the data will be stored.\n",
    "5. **Storage Account Path:** The path of the storage account where the data will be stored.\n",
    "6. **Partition List:** Specifies if any partition is required. If not, provide an empty list.\n",
    "\n",
    "## Notes:\n",
    "\n",
    "1. Always put the column names list, Schema list and Nullable list (whenever used) in same orders\n",
    "2. To assign a null parameter to any specific column, define a list of values based on the column list provided.\n",
    "3. If storing in built-in storage or Hive, no need to mount the storage. If storing in an external storage, configure the mount point and provide the complete path to store the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe021644-f9e4-478e-be60-81e97beea8d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inputs for Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "261c7ca7-5d23-4d45-ba48-c7f722b220b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create table command inputs  \n",
    "column_name=[]                                                  #Input for Target table columns in an array list with the required order\n",
    "table_schema=[]                                                 #Input for Target table schema same as orders of Column list, in array format\n",
    "nullable=[]                                                     #Input for Target table schema same as orders of Column list, in array format\n",
    "target_table_name=\"\"                                            #Input for Target table name which we want\n",
    "path=''                                                         #Input for Target table path here we want to store our data \n",
    "partition_column=[]                                             #Input for Target table partition columns if required, in form of array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd0a6ff-a674-4758-8f48-098952868466",
     "showTitle": true,
     "title": "Example parameter"
    }
   },
   "outputs": [],
   "source": [
    "# # Create table command inputs  \n",
    "# column_name=['emp','id']                                                  #Input for Target table columns in an array list with the required order\n",
    "# table_schema=['string','int']                                             #Input for Target table schema same as orders of Column list, in array format\n",
    "# nullable=[]                                                               #Input for Target table schema same as orders of Column list, in array format\n",
    "# target_table_name=\"abc\"                                                   #Input for Target table name which we want\n",
    "# path='dbfs:/FileStore/tables/abc'                                         #Input for Target table path here we want to store our data\n",
    "# partition_column=[]                                                       #Input for Target table partition columns if required, in form of array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb084f42-ce9e-4cd2-852a-4ea0d9418fd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Table command automation in Sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248df3cf-b83f-40a7-b058-c1acbe3e6daa",
     "showTitle": true,
     "title": "If we are sure Null array wont be empty at any point if time "
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%sql\n CREATE TABLE IF NOT EXISTS abc \n ( \nemp string not null, \nid int \n) \n USING DELTA \n LOCATION  'dbfs:/FileStore/tables/abc' PARTITIONED BY (id) \n TBLPROPERTIES (delta.autoOptimize.autoCompact=true,delta.autoOptimize.optimizeWrite=true)\n"
     ]
    }
   ],
   "source": [
    "b=''\n",
    "p=''\n",
    "max_len=len(column_name)\n",
    "pmax_len=len(partition_column)\n",
    "\n",
    "if(len(column_name)==len(table_schema)==len(nullable)):              #Checks if the All 3 list have prper values or not\n",
    "    for i in range(0,len(column_name)):\n",
    "        b+=column_name[i]+' '+table_schema[i]\n",
    "        if(i!=max_len-1):                                            #Used to remove the end ',' post the column and schema declaration\n",
    "            if(nullable[i]==''):                                     #Used to identify the nullable and non nullable columns \n",
    "                b+=\", \\n\"\n",
    "            else:\n",
    "                b+=\" \"+nullable[i]+', \\n'\n",
    "        else:\n",
    "            if(nullable[i]==''):\n",
    "                b+=\" \\n\"\n",
    "            else:\n",
    "                b+=\" \"+nullable[i]+' \\n'\n",
    "    if (len(partition_column)!=0):                                   #Used to identify if we have to do partition or not \n",
    "        for i in range(len(partition_column)):\n",
    "                if(i!=pmax_len-1): \n",
    "                    p+=partition_column[i]+\",\"\n",
    "                else:\n",
    "                    p+=partition_column[i]\n",
    "        print(\"%sql\\n CREATE TABLE IF NOT EXISTS \"+target_table_name,\"\\n ( \\n\"+b+\") \\n USING DELTA \\n LOCATION \",\"'\"+path+\"'\"\\\n",
    "        ,\"PARTITIONED BY (\"+p+\")\",\"\\n TBLPROPERTIES (delta.autoOptimize.autoCompact=true,delta.autoOptimize.optimizeWrite=true)\")\n",
    "    else:\n",
    "        print(\"%sql\\n CREATE TABLE IF NOT EXISTS \"+target_table_name,\"\\n ( \\n\"+b+\") \\n USING DELTA \\n LOCATION \",\"'\"+path+\"'\"\\\n",
    "        ,\"\\n TBLPROPERTIES (delta.autoOptimize.autoCompact=true,delta.autoOptimize.optimizeWrite=true)\")\n",
    "else:\n",
    "    print(\"Need proper inputs as one of the list is having less size \\n\",\"Column name list-\",len(column_name),\"\\n schema list-\",len(table_schema),\"\\n Nullable-\",len(nullable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36aeb17f-d262-4b27-afb1-82c315daeabf",
     "showTitle": true,
     "title": "If we are sure Null array is empty at any point if time"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%sql\n CREATE TABLE IF NOT EXISTS abc \n ( \nemp string False, \nid int False \n) \n USING DELTA \n LOCATION  'dbfs:/FileStore/tables/abc' PARTITIONED BY (id) \n TBLPROPERTIES (delta.autoOptimize.autoCompact=true,delta.autoOptimize.optimizeWrite=true)\n"
     ]
    }
   ],
   "source": [
    "b=''\n",
    "p=''\n",
    "max_len=len(column_name)\n",
    "pmax_len=len(partition_column)\n",
    "\n",
    "if(len(column_name)==len(table_schema)):                           #Checks if the All 3 list have prper values or not\n",
    "    for i in range(0,len(column_name)):\n",
    "        b+=column_name[i]+' '+table_schema[i]\n",
    "        if(not nullable):                                          #Used to identify the nullable array is empty otr not as it define the primary key set  either it should be empty or it should be full array for all columns \n",
    "            if(i!=max_len-1):                                      #Used to remove the end ',' post the column and schema declaration\n",
    "                b+=\", \\n\"\n",
    "            else:\n",
    "                b+=\" \\n\"\n",
    "        else:\n",
    "            if(i!=max_len-1):\n",
    "                if(nullable[i]==''):                               #Used to identify the nullable and non nullable columns\n",
    "                    b+=\", \\n\"\n",
    "                else:\n",
    "                    b+=\" \"+nullable[i]+', \\n'\n",
    "            else:\n",
    "                if(nullable[i]==''):\n",
    "                    b+=\" \\n\"\n",
    "                else:\n",
    "                    b+=\" \"+nullable[i]+' \\n'\n",
    "    if (len(partition_column)!=0):                                 #Used to identify if we have to do partition or not \n",
    "        for i in range(len(partition_column)):\n",
    "                if(i!=pmax_len-1): \n",
    "                    p+=partition_column[i]+\",\"\n",
    "                else:\n",
    "                    p+=partition_column[i]\n",
    "        print(\"%sql\\n CREATE TABLE IF NOT EXISTS \"+target_table_name,\"\\n ( \\n\"+b+\") \\n USING DELTA \\n LOCATION \",\"'\"+path+\"'\"\\\n",
    "        ,\"PARTITIONED BY (\"+p+\")\",\"\\n TBLPROPERTIES (delta.autoOptimize.autoCompact=true,delta.autoOptimize.optimizeWrite=true)\")\n",
    "    else:\n",
    "        print(\"%%sql\\n CREATE TABLE IF NOT EXISTS \"+target_table_name,\"\\n ( \\n\"+b+\") \\n USING DELTA \\n LOCATION \",\"'\"+path+\"'\"\\\n",
    "        ,\"\\n TBLPROPERTIES (delta.autoOptimize.autoCompact=true,delta.autoOptimize.optimizeWrite=true)\")\n",
    "else:\n",
    "    print(\"Need proper inputs as one of the list is having less size \\n\",\"Column name list-\",len(column_name),\"\\n schema list-\",len(table_schema))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a263fc34-feac-43c6-8b17-62bb9cc44f47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create table command automation in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e6b093-3b07-40e7-a08d-2478c5bab802",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from delta.tables import *\n \nDeltaTable.createIfNotExists(spark).tableName(\"abc\").addColumn(\"emp\",dataType=\"string\",nullable= False, comment=\"no specific comments for the Column\").addColumn(\"id\",dataType=\"int\",nullable= False, comment=\"no specific comments for the Column\").partitionedBy(\"id\").location(\"dbfs:/FileStore/tables/abc\").execute()\n"
     ]
    }
   ],
   "source": [
    "b=''\n",
    "p=''\n",
    "max_len=len(column_name)\n",
    "pmax_len=len(partition_column)\n",
    "\n",
    "\n",
    "if(not nullable):\n",
    "    print(\"Null array is empty so no code will be generated in the final query \\n \\n\")\n",
    "else:\n",
    "    for i in range(len(nullable)):\n",
    "        if(nullable[i]==\"\" or nullable[i]==\"null\" or nullable[i]==\" \" or nullable[i]==\"Null\" or nullable[i]==\"NULL\"):\n",
    "            nullable[i]=\"True\"\n",
    "        else:\n",
    "            nullable[i]=\"False\"\n",
    "if(len(column_name)==len(table_schema)):                            #Checks if the All 3 list have prper values or not\n",
    "    for i in range(len(column_name)):\n",
    "        b+='.addColumn(\"'+column_name[i]+'\",dataType=\"'+table_schema[i]+'\"'\n",
    "        if(not nullable):                                           #Used to identify the nullable array is empty otr not as it define the primary key set  either it should be empty or it should be full array for all columns \n",
    "            b+=', comment=\"no specific comments for the Column\")'\n",
    "        else:\n",
    "            b+=',nullable= '+nullable[i]+', comment=\"no specific comments for the Column\")'\n",
    "    if (len(partition_column)!=0):                                  #Used to identify if we have to do partition or not \n",
    "        for i in range(len(partition_column)):\n",
    "                if(i!=pmax_len-1): \n",
    "                    p+='\"'+partition_column[i]+'\",'\n",
    "                else:\n",
    "                    p+='\"'+partition_column[i]+'\"'\n",
    "        print('from delta.tables import *\\n \\n'+'DeltaTable.createIfNotExists(spark)'+'.tableName(\"'+target_table_name+'\")'+b+\".partitionedBy(\"+p+\")\"+'.location(\"'+path+'\")'+'.execute()' )\n",
    "    else:\n",
    "        print('from delta.tables import *\\n \\n'+'DeltaTable.createIfNotExists(spark)'+'.tableName(\"'+target_table_name+'\")'+b+'.location(\"'+path+'\")'+'.execute()' )\n",
    "else:\n",
    "    print(\"Need proper inputs as one of the list is having less size \\n\",\"Column name list-\",len(column_name),\"\\n schema list-\",len(table_schema))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Automatization_Script (Create Table command )",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
